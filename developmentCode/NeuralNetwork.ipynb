{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, num_hidden, epochs, learning_rate, num_nodes_layers, activation_function, batch_size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "\n",
    "        self.num_data = np.shape(x)[1]  # no. of data points    # no. of rows\n",
    "        self.k = np.shape(x)[0]  # no. of features   # no. of cols\n",
    "        self.n_out = np.shape(y)[0]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_hidden + 1  # +1 for output layer\n",
    "\n",
    "        self.num_nodes_layers = num_nodes_layers\n",
    "\n",
    "        # inserting input and output nodes to the list\n",
    "        self.num_nodes_layers.insert(0, self.k)\n",
    "        self.num_nodes_layers.append(self.n_out)\n",
    "\n",
    "        self.leaky_slope = 0.01\n",
    "        self.weights = []\n",
    "        \n",
    "        # parameters: weight and bias\n",
    "        # weight[l] : (num_layers * num_layers-1 ) * num_layers : (no. of nodes in layer l * no. of nodes in layer (l-1)) * no. of layers\n",
    "    def initialize_parameters_random(self):\n",
    "\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.weights.append(\n",
    "                np.random.rand(self.num_nodes_layers[l], self.num_nodes_layers[l - 1]))\n",
    "\n",
    "    # Use this when activation function is tanh or sigmoid\n",
    "    def initialize_parameters_xavier(self):\n",
    "\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.weights.append(np.random.randn(self.num_nodes_layers[l], self.num_nodes_layers[l - 1]) * np.sqrt(\n",
    "                1 / self.num_nodes_layers[l - 1]))\n",
    "\n",
    "    # Use this when activation function is ReLU or Leaky ReLu\n",
    "    def initialize_parameters_he(self):\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.weights.append(np.random.randn(self.num_nodes_layers[l], self.num_nodes_layers[l - 1]) * np.sqrt(\n",
    "                2 / self.num_nodes_layers[l - 1]))\n",
    "\n",
    "    # Activation Functions\n",
    "    def activation(self, x):\n",
    "        if self.activation_function == \"linear\":\n",
    "            return x\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        if self.activation_function == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        if self.activation_function == \"relu\":\n",
    "            a = np.zeros_like(x)\n",
    "            return np.maximum(a, x)\n",
    "        if self.activation_function == \"leaky_relu\":\n",
    "            a = self.leaky_slope * x\n",
    "            return np.maximum(a, x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def gradient_activation(self, X):\n",
    "        if self.activation_function == \"linear\":\n",
    "            return np.ones_like(X)\n",
    "        elif self.activation_function == \"sigmoid\":\n",
    "            return self.activation(X) * (1 - self.activation(X))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return (1 - np.square(X))\n",
    "        elif self.activation_function == \"relu\":\n",
    "            grad = np.zeros_like(X)\n",
    "            grad[X > 0] = 1.0\n",
    "            return grad\n",
    "        elif self.activation_function == \"leaky_relu\":\n",
    "            grad = np.ones_like(X)\n",
    "            grad[X <= 0] = self.leaky_slope\n",
    "            return grad\n",
    "\n",
    "    def forward_propogation(self, x):\n",
    "        # dim of A vector: (no. of hidden nodes * num_data) *(no. of layers)\n",
    "        A = []\n",
    "        Z = []\n",
    "        A.append(x)\n",
    "        A_prev = x\n",
    "\n",
    "        for l in range(0, self.num_layers-1):\n",
    "            z = np.matmul(self.weights[l], A_prev)\n",
    "            a = self.activation(z)\n",
    "            A_prev = a\n",
    "            A.append(a)\n",
    "            Z.append(z)\n",
    "        z = np.matmul(self.weights[-1], A_prev)\n",
    "        # ******* Can apply different activation to differnt nodes in last layer?****\n",
    "        a = self.activation(z)\n",
    "        A.append(a)\n",
    "        Z.append(z)\n",
    "        return (A, Z)\n",
    "\n",
    "    def back_propogation(self, A, Z, y):\n",
    "\n",
    "        delta_z = [None for i in range(self.num_layers)]\n",
    "        delta_weight = [None for i in range(self.num_layers)]\n",
    "\n",
    "        delta_z[-1] = (y - A[-1])\n",
    "        delta_weight[-1] = np.matmul(delta_z[-1], A[-2].T)\n",
    "\n",
    "        for l in range(self.num_layers - 2, -1, -1):\n",
    "            delta_z[l] = np.multiply(np.matmul(self.weights[l + 1].T, delta_z[l + 1]), self.gradient_activation(Z[l]) )\n",
    "            delta_weight[l] = np.matmul( delta_z[l], A[l].T )\n",
    "\n",
    "        return delta_weight\n",
    "\n",
    "\n",
    "    def update_weight(self, A, delta_weight):\n",
    "        # weight = weight + learning_rate * error * input\n",
    "        m = A[-1].shape[1]\n",
    "        for l in range(self.num_layers):\n",
    "            self.weights[l] = self.weights[l] + (self.learning_rate * delta_weight[l])/m\n",
    "\n",
    "    def predict(self, x_test, isMissing):\n",
    "        A,Z = self.forward_propogation(x_test)\n",
    "        prediction = A[-1]\n",
    "        predFinal = np.where(isMissing < 1, prediction, x_test)\n",
    "        return predFinal\n",
    "\n",
    "    def loss_function(self, y, out):\n",
    "#             return (0.5 * np.mean((y - out) ** 2))\n",
    "        return (np.mean(np.sum((y - out) ** 2, axis = 1)))\n",
    "\n",
    "    def model(self):\n",
    "        mini_batch = int((self.num_data) / (self.batch_size))\n",
    "        \n",
    "        self.initialize_parameters_he()\n",
    "        \n",
    "#         if self.activation_function == \"linear\":\n",
    "#             self.initialize_parameters_random()\n",
    "#         elif self.activation_function == \"sigmoid\" or self.activation_function == \"tanh\":\n",
    "#             self.initialize_parameters_xavier()\n",
    "#         else:\n",
    "#             self.initialize_parameters_he()\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "\n",
    "            print(\"Epoch =\", e)\n",
    "            end = 0\n",
    "            for n in range(mini_batch + 1):\n",
    "\n",
    "                if (n != mini_batch):\n",
    "                    start = n * self.batch_size\n",
    "                    end = (n + 1) * self.batch_size\n",
    "                    x_ = self.x[:, start:end]\n",
    "                    y_ = self.y[:, start:end]\n",
    "\n",
    "                else:\n",
    "                    if ((self.num_data % self.batch_size) != 0):\n",
    "                        x_ = self.x[:, end:]\n",
    "                        y_ = self.y[:, end:]\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                A,Z = self.forward_propogation(x_)\n",
    "                delta_weight = self.back_propogation(A, Z, y_)\n",
    "                self.update_weight(A, delta_weight)\n",
    "\n",
    "                loss = self.loss_function(A[-1], y_)\n",
    "\n",
    "            print(\"loss = \", loss)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('Data/Train/X.npy')\n",
    "X_prime_train = np.load('Data/Train/X_prime.npy')\n",
    "feature_info_train = np.load('Data/Train/feature_information.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('Data/Test/X.npy')\n",
    "X_prime_test = np.load('Data/Test/X_prime.npy')\n",
    "feature_info_test = np.load('Data/Test/feature_information.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = X_prime_train.T\n",
    "trainY = X_train.T\n",
    "testX = X_prime_test.T\n",
    "testY = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 30162)\n",
      "(106, 15060)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\n",
      "loss =  0.03026098204572844\n",
      "Epoch = 1\n",
      "loss =  0.031515579216232485\n",
      "Epoch = 2\n",
      "loss =  0.031891478778654093\n",
      "Epoch = 3\n",
      "loss =  0.03206930559648685\n",
      "Epoch = 4\n",
      "loss =  0.031721469834786326\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(trainX, trainY, num_hidden= 1, epochs= 5, learning_rate=0.01, num_nodes_layers=[10],\n",
    "                       activation_function=\"tanh\", batch_size = 1)\n",
    "nn.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.68455750e-02, -1.50436359e-02, -1.61787211e-02, ...,\n",
       "         4.19256436e-02,  1.01633094e-01, -4.52208378e-02],\n",
       "       [-1.39532279e-02,  7.56963579e-03, -6.91640883e-02, ...,\n",
       "        -1.97108010e-02, -1.10134761e-01, -6.09950407e-02],\n",
       "       [ 1.65501999e-02, -1.34529383e-02,  2.17425447e-05, ...,\n",
       "         1.43382728e-02,  1.03637190e-02, -2.09316627e-02],\n",
       "       ...,\n",
       "       [-2.01002139e-02, -4.64195135e-03, -7.80183340e-03, ...,\n",
       "         1.23410790e-01, -7.35299057e-02, -8.55047242e-03],\n",
       "       [ 4.00907446e-03,  3.36051736e-03,  4.21666089e-02, ...,\n",
       "         8.62435832e-03,  1.79551590e-01, -1.74776994e-03],\n",
       "       [ 1.35841362e-03, -1.83141620e-03,  2.05965573e-03, ...,\n",
       "        -1.09133804e-02,  1.28614162e-01,  2.42154018e-02]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nn.predict(testX, feature_info_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing error: 6.465487038866718\n"
     ]
    }
   ],
   "source": [
    "testErr = np.sqrt(np.mean(np.sum((pred - testY) ** 2, axis = 1)))\n",
    "# testErr = nn.loss_function(pred, testY)\n",
    "print(\"Testing error:\", testErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.150685\n",
       "1     0.219011\n",
       "2     0.733333\n",
       "3     0.000000\n",
       "4     0.000000\n",
       "5     0.397959\n",
       "6     0.000000\n",
       "7     1.000000\n",
       "8     0.000000\n",
       "9     0.000000\n",
       "10    0.000000\n",
       "11    0.000000\n",
       "12    0.000000\n",
       "13    0.000000\n",
       "14    0.000000\n",
       "15    0.000000\n",
       "16    0.000000\n",
       "17    0.000000\n",
       "18    0.000000\n",
       "19    0.000000\n",
       "20    1.000000\n",
       "21    0.000000\n",
       "22    0.000000\n",
       "23    0.000000\n",
       "24    0.000000\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df.iloc[:25,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.150685\n",
       "1     0.219011\n",
       "2     0.733333\n",
       "3     0.000000\n",
       "4     0.000000\n",
       "5     0.397959\n",
       "6     0.000000\n",
       "7     1.000000\n",
       "8     0.000000\n",
       "9     0.000000\n",
       "10    0.000000\n",
       "11    0.000000\n",
       "12    0.000000\n",
       "13    0.000000\n",
       "14    0.000000\n",
       "15    0.000000\n",
       "16    0.000000\n",
       "17    0.000000\n",
       "18    0.000000\n",
       "19    0.000000\n",
       "20    1.000000\n",
       "21    0.000000\n",
       "22    0.000000\n",
       "23    0.000000\n",
       "24    0.000000\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX_df = pd.DataFrame(testX)\n",
    "testX_df.iloc[:25,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.150685\n",
       "1     0.219011\n",
       "2     0.733333\n",
       "3     0.000000\n",
       "4     0.000000\n",
       "5     0.397959\n",
       "6     0.000000\n",
       "7     1.000000\n",
       "8     0.000000\n",
       "9     0.000000\n",
       "10    0.000000\n",
       "11    0.000000\n",
       "12    0.000000\n",
       "13    0.000000\n",
       "14    0.000000\n",
       "15    0.000000\n",
       "16    0.000000\n",
       "17    0.000000\n",
       "18    0.000000\n",
       "19    0.000000\n",
       "20    1.000000\n",
       "21    0.000000\n",
       "22    0.000000\n",
       "23    0.000000\n",
       "24    0.000000\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_df = pd.DataFrame(testY)\n",
    "testY_df.iloc[:25,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     0\n",
       "4     0\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "11    1\n",
       "12    1\n",
       "13    1\n",
       "14    1\n",
       "15    1\n",
       "16    0\n",
       "17    1\n",
       "18    1\n",
       "19    1\n",
       "20    1\n",
       "21    0\n",
       "22    1\n",
       "23    1\n",
       "24    0\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_info_test_df = pd.DataFrame(feature_info_test)\n",
    "feature_info_test_df.iloc[:25,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
